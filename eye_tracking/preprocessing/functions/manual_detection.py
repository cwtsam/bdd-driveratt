from pupil_apriltags import Detector, Detection
from collections import defaultdict
from typing import List, Dict, Tuple, Any
from glob import glob
import os
import time
import sys
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from PIL import Image
import numpy as np
import cv2
import pandas as pd


def extract_frames(video_path: str, frames_path: str) -> None:
    """Convert a video (mp4 or similar) into a series of individual PNG frames.
    Make sure to create a directory to store the frames before running this function.
    Args:
        video_path (str): filepath to the video being converted
        frames_path (str): filepath to the target directory that will contain the extract frames
    """
    video = cv2.VideoCapture(video_path)
    count = 0
    success = 1

    # Basically just using OpenCV's tools
    while success:
        success, frame = video.read()
        cv2.imwrite(f'{frames_path}/frame{count}.png', frame)
        count += 1

    # Optional print statement
    print(f'Extracted {count} frames from {video_path}.')


def detect_tags(frames_path: str, aperture=11, visualize=False) -> Tuple[List[List[Dict[str, Any]]], List[list], Dict[int, int]]:
    """Detect all tags (Apriltags3) found in a folder of PNG files and return (1) a list of tag objects
    for preprocessing and (2) a dictionary containing the frequency that each tag ID appeared
    Args:
        frames_path (str): path to the directory containing PNG images
        aperture (int):
        visualize (bool):

    Returns:
        frames (List[Dict[str, Any]]): list of objects containing id (int), centroid (np.array[int]) and corners (np.array[int])
        corners (List): list of corner coordinates
        tag_ds (Dict[int, int]): dictionary mapping tag IDs to frequency of tag across all images
    """
    # Initialize variables
    frames = []
    tag_ids = defaultdict(int)
    at_detector = Detector()
    corners = []

    # Sort by index in.../frame<index>.png
    all_images = sorted(glob(f'{frames_path}/*.png'), key=lambda f: int(os.path.basename(f)[5:-4]))

    # Deleted last image after out of range error popped up
    # TODO: but not analyzing last 2 frames?
    # print(len(all_images))
    if len(all_images) > 1:
        all_images = all_images[:-1]

    num_images = len(all_images)
    # print_progress_bar(0, num_images, prefix='Progress:', suffix='Complete', length=50)

    # Iterate thru all PNG images in frames_path
    for i, img_path in enumerate(all_images):
        # Create a grayscale 2D NumPy array for Detector.detect()
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)

        if type(img) == np.ndarray:
            tags_in_framex = []
            for tag in at_detector.detect(img):
                # Increment frequency
                tag_ids[tag.tag_id] += 1
                # r = np.roll(np.float32(img), tag.homography + 1, axis=0)
                # Add to frames in following format - feel free to adjust
                tags_in_framex.append({
                    'id': tag.tag_id,
                    'id_confidence': tag.decision_margin,
                    'soft_id': tag.tag_id,
                    'perimeter': 100, #cv2.arcLength(img, closed=True),
                    'centroid': tag.center,
                    'verts': tag.corners,
                    'frames_since_true_detection': 0
                })

                # {'id': msg, 'id_confidence': id_confidence, 'verts': r.tolist(), 'soft_id': soft_msg,
                #  'perimeter': cv2.arcLength(r, closed=True), 'centroid': centroid.tolist(),
                #  "frames_since_true_detection": 0}
            frames.append(tags_in_framex)
        # create bounding box
        corners.append(bounding_box(frames_path, frames, img_path))
        time.sleep(0.01)
        print_progress_bar(i + 1, num_images, prefix='Progress:', suffix='Complete', length=50)

    return frames, corners, dict(tag_ids)

#function that gets what attribute is of the dictionary generated by detect tags a user needs
#for instance, I needed the centers of each tag and the corners to crop the image, so this function acquires tgem from the
#return call of detect tags
def attribute(frame, feature):
    # qr_codes = frame[0]
    qr_codes = frame
    attributes = []
    for i in range(len(qr_codes)):
        qr_code = qr_codes[i]
        # print(feature + ' of tag id ' + str(i) + ":", qr_code[feature])
        attributes.append(qr_code[feature])
    return attributes
    # return frame[feature]

# def bounding_box(folder):
def bounding_box(folder, frame, img_path):
    # Create frame path using OS package
    # Define the name of the directory to be created
    bounding_box_frames_path = folder + "/bounding_box_frames"
    try:
        if not os.path.exists(bounding_box_frames_path):
            os.mkdir(bounding_box_frames_path)
            print("Successfully created the directory %s " % bounding_box_frames_path)
            # extract_frames(video_path, frames_path)
        else:
            print("Directory %s already exists." % bounding_box_frames_path)
    except OSError:
        print("Creation of the directory %s failed" % bounding_box_frames_path)

    for f in frame:

        # attains the corners for each of the QR codes in the frame; not sorted in any order
        corners = attribute(f, 'verts')
        # bottom left corners[x][0]
        # bottom right corners[x][1]
        # top right corners[x][2]
        # top left corners[x][3]

        if len(corners) > 0:
            top_left = corners[0][3]
            left = corners[1][3]
            bottom_left = corners[2][0]
            bottom_center_left = corners[3][0]
            bottom_center_right = corners[4][0]
            bottom_right = corners[5][1]
            right = corners[6][1]
            top_right = corners[7][2]
            top_center_right = corners[8][2]
            top_center_left = corners[9][2]

            coordinates = [top_left, left, bottom_left, bottom_center_left, bottom_center_right,
                        bottom_right, right, top_right, top_center_right, top_center_left]
        else:
            # take prev frame's coordinates
            drawing_pts = coordinates
            coordinates = []
    # check if last f has coordinates
    # if not, then take last coordinates that did exist but it may not be the last direct frame but a few frames down

    img = cv2.imread(img_path, cv2.IMREAD_COLOR)
    # take prev frame's coordinates
    # pts = np.array(drawing_pts, np.int32)
    pts = np.array(coordinates, np.int32)
        # pts = np.array([top_left, bottom_left, bottom_right, top_right], np.int32)
    pts = pts.reshape((-1, 1, 2))
    cv2.polylines(img, [pts], True, (0, 255, 255), 3)
    head, tail = os.path.split(img_path)
    cv2.imwrite(bounding_box_frames_path + "/" + tail, img)

    return coordinates


def print_progress_bar (iteration, total, prefix ='', suffix ='', decimals = 1, length = 100, fill ='â–ˆ', printEnd ="\r"):
    """
    Call in a loop to create terminal progress bar
    @params:
        iteration   - Required  : current iteration (Int)
        total       - Required  : total iterations (Int)
        prefix      - Optional  : prefix string (Str)
        suffix      - Optional  : suffix string (Str)
        decimals    - Optional  : positive number of decimals in percent complete (Int)
        length      - Optional  : character length of bar (Int)
        fill        - Optional  : bar fill character (Str)
        printEnd    - Optional  : end character (e.g. "\r", "\r\n") (Str)
    """
    percent = ("{0:." + str(decimals) + "f}").format(100 * (iteration / float(total)))
    filledLength = int(length * iteration // total)
    bar = fill * filledLength + '-' * (length - filledLength)
    sys.stdout.write('\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix))
    # Print New Line on Complete
    if iteration == total:
        print()